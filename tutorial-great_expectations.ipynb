{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Great Expectations tutorial\n",
    "\n",
    "TODO: update the following paragraph as the project evolves\n",
    "\n",
    "Welcome to this hands-on tutorial on Great Expectations! We'll show you why and how to use Great Expectations to enhance the quality of your data. We'll first introduce you to the tool and show you how to get started. Then we'll go over expectations and expectation suites, which are the key building blocks to test your data. We'll show you how to generate beautiful reports on your data. We'll then build a checkpoint, and finally introduce you to more advanced stuff.\n",
    "\n",
    "### What is Great Expectations exactly?\n",
    "\n",
    "Great Expectations is a tool that allows you to test batch data. It generates reports about the data, containing documentation of the data translated from the definitions of the tests.\n",
    "\n",
    "### Why you should use it: data quality\n",
    "\n",
    "You should test your data for two main reasons:\n",
    "- better data quality leads to better predictions and insights relying on the data,\n",
    "- it's an additional way to test data pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's jump into it then!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we'll need a `DataContext`. This represents a Great Expectations project, holding all your configurations, expectation suites, data sources and so on. We'll have a better look at the data context later [[jump ahead]](#section-data-context), but just to get started we shipped a simple one with this tutorial.\n",
    "\n",
    "We'll load that one right now. By default, Great Expectations will look for your configuration in the `great_expectations` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ge.data_context.DataContext()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our `DataContext` ready, we can add an expectation suite. Think of this like a test suite, but for your data instead of for your code. Usually you'll do this through the CLI, but we will get to that later [[jump ahead]](#section-cli). We'll name the suite `check_avocado_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = context.create_expectation_suite(\n",
    "    \"check_avocado_data\",\n",
    "    overwrite_existing=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load our dataset, `avocado.csv`, from our data context. This involves a bit of configuration, but don't worry about it too much for now. We'll get back to that later [TODO: when + link]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_kwargs = {\n",
    "      'path': 'data/avocado.csv',\n",
    "      'datasource': 'data_dir',\n",
    "      'reader_method': 'read_csv',\n",
    "      'data_asset_name': 'avocado',\n",
    "}\n",
    "batch = context.get_batch(batch_kwargs, suite)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, that's it for setup!\n",
    "\n",
    "Let's continue to our avocado sales data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the documentation that came with the data:\n",
    " - Date - The date of the observation\n",
    " - AveragePrice - the average price of a single avocado\n",
    " - type - agriculture type: conventional or organic\n",
    " - Region - the city or region of the observation\n",
    " - Total Volume - Total number of avocados sold\n",
    " - 4046 - Total number of avocados with PLU 4046 sold (small Hass)\n",
    " - 4225 - Total number of avocados with PLU 4225 sold (large Hass)\n",
    " - 4770 - Total number of avocados with PLU 4770 sold (extra large Hass)\n",
    " \n",
    "These descriptions sure help us to understand the dataset a bit better, but they don't exactly provide much guarantees. When consuming this dataset, what assumptions can we make? Will the `region` field always be specified? Will the `Date` field always be in the same format? Those sales counts, are they supposed to add up?\n",
    "\n",
    "Great Expectations helps us to codify these properties in a set of `Expectations`. An `Expectation` is, well, something that you expect to be true in your data. Again, think of it like an unit test for your dataset.\n",
    "\n",
    "Let's run a basic `Expectation` to get started. For example, we could check whether the `Date` column is present in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"success\": true,\n",
       "  \"result\": {},\n",
       "  \"meta\": {},\n",
       "  \"exception_info\": {\n",
       "    \"raised_exception\": false,\n",
       "    \"exception_traceback\": null,\n",
       "    \"exception_message\": null\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.expect_column_to_exist('Date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting `dict` we got back might feel a bit weird at first, but you'll see later on how this output is used to generate reports [[jump ahead]](#section-data-docs). For now, just note that `success` has the value `true`, indicating that our expectation passed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a simple check that only assesses the data shape, but doesn't touch the values in there (it is a _table-level check_).\n",
    "\n",
    "Let's try adding a check for the values now. Maybe we can address one of the concerns we raised: can we add an `Expectation` that ensures every record will have its `region` specified?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.expect_column_values_to_not_be_null('region')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That worked! This time we got a bit more info back: the `result` section now contains some metrics about our data. We can see that all 18249 records passed the check, and there were no unexpected (i.e. `null`) values. If Great Expectations finds any offending values, they will be listed in the `partial_unexpected_list`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do something that's a bit more strict. It would be nice, for example, to make sure that all `region`s are actually strings, so that we don't end up with numeric regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.expect_column_values_to_be_of_type('region', 'str')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that metrics on the amount of missing values were still collected. This way, we can disambiguate between missing values and incorrect values. In case you were wondering, the `unexpected_percent_nonmissing` refers to the percentage of present (non-null) values that did not meet our expectation (they were not a string). If other metrics are unclear to you, check out [this documentation page](https://docs.greatexpectations.io/en/latest/reference/core_concepts/expectations/result_format.html#behavior-for-summary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we covered the basics, let's get to some fancier expectations. For example, we could make sure that all `Date`s are in the expected format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.expect_column_values_to_match_strftime_format('Date', \"%Y-%m-%d\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example: we can make sure all the listed avocado prices are reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.expect_column_values_to_be_between('AveragePrice', min_value=0.5, max_value=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops! That failed. Looks like we have some outliers here! Great Expectations helpfully collected them for us. By default, it will collect up to 20 examples of values that didn't meet the expectation (that's why it's called the _partial_ unexpected list).\n",
    "\n",
    "If we want to allow these outliers, we can add some tolerance to the check by using the `mostly` parameter. Let's settle for having 99% of avocados being priced within the range we specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.expect_column_values_to_be_between('AveragePrice', min_value=0.5, max_value=3.0, mostly=0.99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common usecase would be when you only expect a certain set of values to show up in a column. This is the case for our `type` column, since we only know about `conventional` and `organic` grown avocados. Let's add a check for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.expect_column_distinct_values_to_be_in_set('type', ['conventional', 'organic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could even add a check on the value frequencies! For example, if we want the ratio of organic to conventional to be roughly equal, we could check the [Kullback-Leiber divergence](https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence) between our assumed distribution, and the one that is observed in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_object = {\n",
    "    'values': ['conventional', 'organic'],\n",
    "    'weights': [0.5, 0.5],\n",
    "    \n",
    "}\n",
    "batch.expect_column_kl_divergence_to_be_less_than('type', partition_object, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we checked out some expectations, maybe try adding one yourself? You can check out the [glossary of expectations](https://docs.greatexpectations.io/en/latest/reference/glossary_of_expectations.html) for a complete list of what you can do. Go wild!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The stage is all yours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Expectation Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, while we were experimenting up there, great_expectations remembered all the expectations we ran. We can now retrieve the suite contents as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch.get_expectation_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gave us the `dict` representation Great Expectations uses under the hood to keep track of our exepectation suite. Can you recognise some of the expectations we wrote?\n",
    "\n",
    "This representation can then be saved to a file, so that we can load it again at another time, without depending on the python code that produced it.\n",
    "\n",
    "Note that by default, expectations that failed on the `batch` we ran them against will be omitted. If you want to include them anyways, you could add the `discard_failed_expectations=False` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch.save_expectation_suite()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us get back to that configuration we mentioned earlier. As we said, it's just some files living in the `great_expectations` directory. This is what it looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[01;34mgreat_expectations\u001b[0m\r\n",
      "├── \u001b[01;34mcheckpoints\u001b[0m\r\n",
      "│   └── avocado_data.yml\r\n",
      "├── \u001b[01;34mexpectations\u001b[0m\r\n",
      "│   ├── check_avocado_data.json\r\n",
      "│   └── \u001b[01;34mdata_dir\u001b[0m\r\n",
      "│       └── \u001b[01;34msubdir_reader\u001b[0m\r\n",
      "│           └── \u001b[01;34mavocado\u001b[0m\r\n",
      "│               └── BasicDatasetProfiler.json\r\n",
      "├── great_expectations.yml\r\n",
      "├── \u001b[01;34mnotebooks\u001b[0m\r\n",
      "│   ├── \u001b[01;34mpandas\u001b[0m\r\n",
      "│   │   └── validation_playground.ipynb\r\n",
      "│   ├── \u001b[01;34mspark\u001b[0m\r\n",
      "│   │   └── validation_playground.ipynb\r\n",
      "│   └── \u001b[01;34msql\u001b[0m\r\n",
      "│       └── validation_playground.ipynb\r\n",
      "└── \u001b[01;34mplugins\u001b[0m\r\n",
      "    └── \u001b[01;34mcustom_data_docs\u001b[0m\r\n",
      "        ├── \u001b[01;34mrenderers\u001b[0m\r\n",
      "        ├── \u001b[01;34mstyles\u001b[0m\r\n",
      "        │   └── data_docs_custom_styles.css\r\n",
      "        └── \u001b[01;34mviews\u001b[0m\r\n",
      "\r\n",
      "14 directories, 8 files\r\n"
     ]
    }
   ],
   "source": [
    "!tree great_expectations -I \"uncommitted\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we have the main `great_expectations.yml` configuration file, a folder with checkpoints, a folder with expectation suites, some playground notebooks, and a folder for plugins. \\\n",
    "[TODO: would it be better to move this part on the configuration folder to the next section?]\n",
    "\n",
    "As you can see, the `save_expectation_suite` command saved our `check_avocado_data` suite to the `expectations` folder! That's all there is to it, the expectation suite is just a file. It contains that same internal representation that we just retrieved. You can check if you like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cat great_expectations/expectations/check_avocado_data.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"validation-results\"></a>\n",
    "Now that we added our expectation suite to our `DataContext`, we can try running the entire suite. This is done by applying a `ValidationOperator` to the suite and the dataset. `ValidationOperator`s for your project are defined in the `great_expectations.yml` file. We already provided a `ValidationOperator` called `action_list_operator` [TODO: would it be clearer to change this name?] which will run the expectation suite and record its results.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = context.run_validation_operator(\"action_list_operator\", assets_to_validate=[batch])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We didn't specify our expectation suite for that command, but remember that the `batch` dataset kept track of our suite for us, so it will know what to do.\n",
    "\n",
    "That produced a big datadump for us. Let's inspect it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is called a *validation result*. Validation results are kept in the *validation store*, which by default is the `great_expectations/uncommitted/validations` directory by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tree great_expectations/uncommitted/validations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great Expectations also allows you to set other backends as a validation store, such as an S3 bucket or a SQL database. Check out [metadata stores](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_metadata_stores.html) if you would like to learn more!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-data-docs\"></a>\n",
    "## Data Docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can render these results to a friendly report, called a data doc. These data docs will describe the expectations that the data should meet, as well as the metrics detailing how well the data meets the requirements. This is how Great Expectations combines testing with documenting. Running the code below will generate the data docs and open them in a new tab, make sure to have a look around. You'll see the code we ran above reflected in the different sections - it's pretty self-explanatory!\n",
    "\n",
    "TODO: make sure these docs are in the repo, and add a link to them for online readers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context.build_data_docs()\n",
    "\n",
    "# get the result identifier for our run\n",
    "validation_result_identifier = list(results[\"run_results\"].keys())[0]\n",
    "context.open_data_docs(validation_result_identifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we exracted that `validation_result_identifier` so we could bring you directly to the data doc for this validation result. When you're out there, you could just as well open the index and navigate to the result yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for validation results, different storage backends can be configured for your data docs. You could, for example, host them on an S3 bucket for easy viewing. Refer to [configuring data docs](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_data_docs.html) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-data-context\"></a>\n",
    "# Data Context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on, let's take a moment to look at the `DataContext`, which represents your Great Expectations setup. As we saw in the previous section, it consists of a directory with a `great_expectations.yml` file, which is the main configuration for your project. We won't focus on all the details here, you can refer to the [data context reference](https://docs.greatexpectations.io/en/latest/reference/spare_parts/data_context_reference.html) for that. Instead, we'll highlight some important concepts:\n",
    "\n",
    "- A **data source** is something that can provide data to Great Expectations, such as an SQL database.\n",
    "- A **data asset** is one dataset that lives in a *data source*, such as an SQL table.\n",
    "\n",
    "In the configuration we provided, there is one *data source* named `data_dir`, which is just a folder with csv files inside. the `avocado.csv` file we are working with would be a *data asset*.\n",
    "More information on data sources can be found in the [data context reference](https://docs.greatexpectations.io/en/latest/reference/spare_parts/data_context_reference.html#datasources).\n",
    "\n",
    "- A **validation operator** specifies what should be done with your validations. Some examples could be writing the validation results to a database, publishing data docs, or sending a notification to a slack channel.\n",
    "    If you'd like to know more you can check out the [validation operators and actions](https://docs.greatexpectations.io/en/latest/reference/core_concepts/validation_operators_and_actions.html) and [how to add a validation operator](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/validation/how_to_add_a_validation_operator.html) documentation pages.\n",
    "\n",
    "\n",
    "- **stores** specify where expectation and validation data will be stored. See [configuring metadata stores](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/configuring_metadata_stores.html) if you're interested.\n",
    "\n",
    "These are all configured in the `great_expectations.yml` file.\n",
    "\n",
    "In addition to those, we still have two important directories:`expectations`, which holds our expectation suites, and `checkpoints`, which we'll check out now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we manually ran our expectation suite against our dataset. While that worked, there is a better way: checkpoints.\n",
    "\n",
    "A checkpoint has three components:\n",
    "- A *data asset* that will be validated\n",
    "- *Expectation suites* to validate the data\n",
    "- A *validation operator* to handle the validation\n",
    "\n",
    "We can create a checkpoint by adding a file in the `checkpoints` directory of our great_expectations configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile great_expectations/checkpoints/avocado_data.yml\n",
    "\n",
    "validation_operator_name: action_list_operator\n",
    "batches:\n",
    "  - batch_kwargs:\n",
    "      path: data/avocado.csv\n",
    "      datasource: data_dir\n",
    "      reader_method: read_csv\n",
    "      data_asset_name: avocado\n",
    "    expectation_suite_names:\n",
    "      - check_avocado_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`batches` is a list of (data batch, expectation suites) pairs. `batch_kwargs` specifies how the data asset should be loaded, you might recognise the parameters from earlier! \\\n",
    "\n",
    "We created the file manually here for demonstration purposes, but when doing this in your own project you probably want to use the CLI [[jump ahead]](#section-cli), which will also help you in setting the right parameters. If you need to configure them further, try [creating batches](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/creating_batches.html).\n",
    "\n",
    "The checkpoint can be executed by using the great_expectations cli:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!great_expectations checkpoint run avocado_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, to summarize: a checkpoint is a _runnable check_ for your data. They are your first stop for integrating Great Expectations into your pipelines and workflows.\n",
    "For more info on how to do that, refer to the [validation guides](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/validation.html), or the [workflows and patterns](https://docs.greatexpectations.io/en/latest/guides/workflows_patterns.html) guides."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous sections we explored how we could get some metrics about our data using expectations. But what if you don't know what exactly to expect of your data? Well, you could try using Great Expectations' profiling feature, which can try to extract some useful metrics from your data. To try profiling our preconfigured `data_dir` data source, we can use the CLI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!great_expectations datasource profile data_dir -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running that command should have presented you with freshly built data docs. You can find the results in the `Profiling Results` tab. The profiler also generated an expectation suite based on its observations, which you can find in the `Expectation Suites` tab. Be mindful that this is an experimental feature and the generated suite is usually not that helpful, but it could be a good starting point for writing your own.\n",
    "\n",
    "\n",
    "If you'd like to know more about profiling, the [profiling reference](https://docs.greatexpectations.io/en/latest/reference/spare_parts/profiling_reference.html) can help you out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"section-cli\"></a>\n",
    "# The Great Expectations CLI\n",
    "\n",
    "For the purposes of this tutorial, we mostly interacted directly with Great Expectations. If you are going to set up and use Great Expectations for yourself, we reccomend using the CLI as much as possible. Refer to the offical [CLI guide](https://docs.greatexpectations.io/en/latest/guides/how_to_guides/miscellaneous/command_line.html) if you need some help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: great_expectations [OPTIONS] COMMAND [ARGS]...\r\n",
      "\r\n",
      "  Welcome to the great_expectations CLI!\r\n",
      "\r\n",
      "  Most commands follow this format: great_expectations <NOUN> <VERB>\r\n",
      "\r\n",
      "  The nouns are: datasource, docs, project, suite, validation-operator\r\n",
      "\r\n",
      "  Most nouns accept the following verbs: new, list, edit\r\n",
      "\r\n",
      "  In particular, the CLI supports the following special commands:\r\n",
      "\r\n",
      "  - great_expectations init : create a new great_expectations project\r\n",
      "\r\n",
      "  - great_expectations datasource profile : profile a datasource\r\n",
      "\r\n",
      "  - great_expectations docs build : compile documentation from expectations\r\n",
      "\r\n",
      "Options:\r\n",
      "  --version      Show the version and exit.\r\n",
      "  -v, --verbose  Set great_expectations to use verbose output.\r\n",
      "  --help         Show this message and exit.\r\n",
      "\r\n",
      "Commands:\r\n",
      "  checkpoint           Checkpoint operations\r\n",
      "  datasource           Datasource operations\r\n",
      "  docs                 Data Docs operations\r\n",
      "  init                 Initialize a new Great Expectations project.\r\n",
      "  project              Project operations\r\n",
      "  store                Store operations\r\n",
      "  suite                Expectation Suite operations\r\n",
      "  validation-operator  Validation Operator operations\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!great_expectations --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install and setup great_expectations\n",
    "\n",
    "Here are a few guidelines if you want to setup everything yourself for your own projects.\n",
    "\n",
    "To install Great Expectations, run `pip install great_expectations` in your terminal. Using a virtual environment is a good practice to install programs with `pip`.\n",
    "\n",
    "To initialize Great Expectations for a project, run `great_expectations init` in your terminal in the project's directory and follow the instructions.\n",
    "\n",
    "For more information on how to set up everything, have a look at https://docs.greatexpectations.io/en/latest/guides/tutorials/getting_started.html and feel free to refer to the official documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
